{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytrec_eval\n",
    "\n",
    "# read the result file and create a dictionary of the form {query_id: {document_id: score}}\n",
    "results = {}\n",
    "with open(YOUR_RESULT_FILE_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        qid, _, docid, _, score, _ = line.strip().split()\n",
    "        if qid not in results:\n",
    "            results[qid] = {}\n",
    "        results[qid][docid] = float(score)\n",
    "\n",
    "# read the qrels file and create a dictionary of the form {query_id: {document_id: relevance_score}}\n",
    "qrels = {}\n",
    "with open(YOUR_QRELS_FILE_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        qid, _, docid, rel = line.strip().split()\n",
    "        if qid not in qrels:\n",
    "            qrels[qid] = {}\n",
    "        qrels[qid][docid] = int(rel)\n",
    "\n",
    "# create an evaluator and compute the evaluation metrics\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map','recall_50', 'P_50'})\n",
    "metrics = evaluator.evaluate(results)\n",
    "\n",
    "# extract P@50 and recall@50 for each query\n",
    "p_50_scores = {}\n",
    "recall_50_scores = {}\n",
    "map_scores = {}\n",
    "for qid, results in metrics.items():\n",
    "    p_50_scores[qid] = results['P_50']\n",
    "    recall_50_scores[qid] = results['recall_50']\n",
    "    map_scores[qid] = results['map']\n",
    "\n",
    "# print the average scores across all queries\n",
    "\n",
    "print(f\"P@50: {sum(p_50_scores.values()) / len(p_50_scores)}\")\n",
    "print(f\"recall@50: {sum(recall_50_scores.values()) / len(recall_50_scores)}\")\n",
    "print(f\"map: {sum(map_scores.values()) / len(map_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
