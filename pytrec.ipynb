import pytrec_eval
import json
# read the result file and create a dictionary of the form {query_id: {document_id: score}}
results = {}
with open(RESULT_FILE_PATH, 'r') as f:
    for line in f:
        qid, _, docid, _, score, _ = line.strip().split()
        if qid not in results:
            results[qid] = {}
        results[qid][docid] = float(score)

# read the qrels file and create a dictionary of the form {query_id: {document_id: relevance_score}}
qrels = {}
with open(QRELS_FILE_PATH, 'r') as f:
    for line in f:
        qid, _, docid, rel = line.strip().split()
        if qid not in qrels:
            qrels[qid] = {}
        qrels[qid][docid] = int(rel)

# create an evaluator and compute the evaluation metrics
evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map','recall_50', 'P_50'})
metrics = evaluator.evaluate(results)

# extract P@50 and recall@50 for each query
p_50_scores = {}
recall_50_scores = {}
map_scores = {}
for qid, results in metrics.items():
    p_50_scores[qid] = results['P_50']
    recall_50_scores[qid] = results['recall_50']
    map_scores[qid] = results['map']

# print the average scores across all queries

print(f"P@50: {sum(p_50_scores.values()) / len(p_50_scores)}")
print(f"recall@50: {sum(recall_50_scores.values()) / len(recall_50_scores)}")
print(f"map: {sum(map_scores.values()) / len(map_scores)}")
